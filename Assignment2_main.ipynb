{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sKEV-sUostG"
      },
      "source": [
        "\n",
        "# COS30049 - Assignment 2\n",
        "### Session 26 Group 2\n",
        "### Swinburne Univeristy of Technology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1.0 Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Verify the DataSet Can Be Read\n",
        "Checks to verify:\n",
        "- File exists\n",
        "- Columns pesent\n",
        "- No missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully!\n",
            "Columns: ['id', 'tweet', 'label']\n",
            "   id                                              tweet label\n",
            "0   1  The CDC currently reports 99031 deaths. In gen...  real\n",
            "1   2  States reported 1121 deaths a small rise from ...  real\n",
            "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
            "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
            "4   5  Populous states can generate large case counts...  real\n"
          ]
        }
      ],
      "source": [
        "file_path = \"Constraint_English_Train.xlsx\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    misinfo_data = pd.read_excel(file_path)\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "    print(\"Columns:\", misinfo_data.columns.tolist())\n",
        "    print(misinfo_data.head())\n",
        "else:\n",
        "    print(\"File not found:\", file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Test Retrieval of Data Matching \"CDC\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows containing 'CDC':\n",
            "        id                                              tweet label\n",
            "0        1  The CDC currently reports 99031 deaths. In gen...  real\n",
            "6        7  If you tested positive for #COVID19 and have n...  real\n",
            "27      28  Just Appendix B gathering all the state orders...  real\n",
            "33      34  CDC Recommends Mothers Stop Breastfeeding To B...  fake\n",
            "138    139  Youth sports organizations: As you resume acti...  real\n",
            "...    ...                                                ...   ...\n",
            "6338  6339  ???The CDC can detain anyone with a fever ??\" ...  fake\n",
            "6345  6346  1645 deaths were reported today bringing the t...  real\n",
            "6377  6378  Acc to @CDCgov &amp; @WHO there is currently n...  real\n",
            "6391  6392  The CDC ???adjusted the US Covid deaths from 1...  fake\n",
            "6405  6406  The cloth face coverings recommended to slow s...  real\n",
            "\n",
            "[281 rows x 3 columns]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/w0/r8xr9pyn4w19x3x9z5x82hm00000gn/T/ipykernel_96455/49314103.py:14: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  matches = misinfo_data.applymap(lambda x: bool(pattern.search(str(x))))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "file_path = \"Constraint_English_Train.xlsx\"\n",
        "misinfo_data = pd.read_excel(file_path)\n",
        "\n",
        "# Compile the regex pattern\n",
        "pattern = re.compile(r'CDC', re.IGNORECASE)  # ignore case if needed\n",
        "\n",
        "# Example: check in a specific column, e.g., 'text'\n",
        "if 'text' in misinfo_data.columns:\n",
        "    matches = misinfo_data['text'].apply(lambda x: bool(pattern.search(str(x))))\n",
        "    print(\"Rows containing 'CDC':\")\n",
        "    print(misinfo_data[matches])\n",
        "else:\n",
        "    # If you want to search all columns\n",
        "    matches = misinfo_data.applymap(lambda x: bool(pattern.search(str(x))))\n",
        "    print(\"Rows containing 'CDC':\")\n",
        "    print(misinfo_data[matches.any(axis=1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Combine data sets\n",
        "- Datasets have been cleaned in Google Refine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/ctip/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/opt/anaconda3/envs/ctip/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/opt/anaconda3/envs/ctip/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_excel(\"Constraint_English_Train_GR.xlsx\")\n",
        "val_df = pd.read_excel(\"Constraint_English_Val_GR.xlsx\")\n",
        "test_df = pd.read_excel(\"Constraint_English_Test_GR.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.0 Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Emoji and Symbols Refining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Emoji and Symbols Detection\n",
        "- Keep symbols used in natural language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 text  has_emoji\n",
            "0         Hello world      False\n",
            "1          Hi there ðŸ˜€       True\n",
            "2  @user_name is cool      False\n",
            "3     No emojis here!      False\n",
            "4            symbol ï¿½       True\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'text': [\"Hello world\", \"Hi there ðŸ˜€\", \"@user_name is cool\", \"No emojis here!\", \"symbol ï¿½\"]\n",
        "})\n",
        "\n",
        "pattern = re.compile(r'[^\\w\\s@.,!?#]', flags=re.UNICODE)\n",
        "df['has_emoji'] = df['text'].apply(lambda x: bool(pattern.search(str(x))))\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test emoji refining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                  tweet  \\\n",
            "0     The CDC currently reports 99031 deaths. In gen...   \n",
            "1     States reported 1121 deaths a small rise from ...   \n",
            "2     Politically Correct Woman (Almost) Uses Pandem...   \n",
            "3     #IndiaFightsCorona: We have 1524 #COVID testin...   \n",
            "4     Populous states can generate large case counts...   \n",
            "...                                                 ...   \n",
            "6415  A tiger tested positive for COVID-19 please st...   \n",
            "6416  Autopsies prove that COVID-19 is a blood clot,...   \n",
            "6417  _A post claims a COVID-19 vaccine has already ...   \n",
            "6418  Aamir Khan Donate 250 Cr. In PM Relief Cares Fund   \n",
            "6419  It has been 93 days since the last case of COV...   \n",
            "\n",
            "                                             clean_text  \n",
            "0     The CDC currently reports 99031 deaths. In gen...  \n",
            "1     States reported 1121 deaths a small rise from ...  \n",
            "2     Politically Correct Woman Almost Uses Pandemic...  \n",
            "3     #IndiaFightsCorona We have 1524 #COVID testing...  \n",
            "4     Populous states can generate large case counts...  \n",
            "...                                                 ...  \n",
            "6415  A tiger tested positive for COVID19 please sta...  \n",
            "6416  Autopsies prove that COVID19 is a blood clot, ...  \n",
            "6417  A post claims a COVID19 vaccine has already be...  \n",
            "6418  Aamir Khan Donate 250 Cr. In PM Relief Cares Fund  \n",
            "6419  It has been 93 days since the last case of COV...  \n",
            "\n",
            "[6420 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "df = pd.DataFrame(train_df)\n",
        "\n",
        "# Step 1: Clean text safely\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    \n",
        "    # Step 1a: Protect @usernames and #hashtags\n",
        "    placeholders = {}\n",
        "    for match in re.findall(r'(@\\w+|#\\w+)', text):\n",
        "        ph = f\"PLACEHOLDER{len(placeholders)}\"\n",
        "        placeholders[ph] = match\n",
        "        text = text.replace(match, ph)\n",
        "    \n",
        "    # Step 1b: Replace all underscores with spaces\n",
        "    text = text.replace('_', ' ')\n",
        "    \n",
        "    # Step 1c: Remove emojis / unusual symbols\n",
        "    # Keep letters, numbers, whitespace, @, #, ., ,, !, ?\n",
        "    text = re.sub(r'[^\\w\\s@.,!?#]', '', text, flags=re.UNICODE)\n",
        "    \n",
        "    # Step 1d: Remove leading punctuation (like . , ! ?) at start of text\n",
        "    text = re.sub(r'^[.,!?_\\s]+', '', text)\n",
        "    \n",
        "    # Step 1e: Restore usernames and hashtags\n",
        "    for ph, original in placeholders.items():\n",
        "        text = text.replace(ph, original)\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Apply cleaning function to the 'tweet' column\n",
        "df['clean_text'] = df['tweet'].apply(clean_text)\n",
        "\n",
        "# Optional: preview result\n",
        "print(df[['tweet', 'clean_text']])\n",
        "\n",
        "# Save cleaned file\n",
        "df.to_excel(\"Constraint_English_Train_Cleaned.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean emojis on all 3 datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Normalized usernames and hashtags instead of keeping them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned dataset saved to Constraint_English_Train_Cleaned.xlsx\n",
            "Cleaned dataset saved to Constraint_English_Val_Cleaned.xlsx\n",
            "Cleaned dataset saved to Constraint_English_Test_Cleaned.xlsx\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/ctip/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/opt/anaconda3/envs/ctip/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Step 1: Define cleaning function\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    \n",
        "    # Step 1a: Normalize links, usernames and hashtags \n",
        "    # Normalize @usernames to <USER>\n",
        "    text = re.sub(r'@\\w+', '<USER>', text)\n",
        "\n",
        "    # Normalize #hashtags to <HASHTAG:topic>\n",
        "    def replace_hashtag(match):\n",
        "        hashtag = match.group()[1:]  # remove #\n",
        "        return f\"<HASHTAG:{hashtag}>\"\n",
        "    \n",
        "    text = re.sub(r'#\\w+', replace_hashtag, text)\n",
        "    \n",
        "    # Normalize links\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '<LINK>', text)\n",
        "    \n",
        "    # Step 1b: Replace all underscores \n",
        "    text = text.replace('_', '')\n",
        "    \n",
        "    # Step 1c: Remove emojis / unusual symbols\n",
        "    # Keep letters, numbers, whitespace, @, #, ., ,, !, ?\n",
        "    text = re.sub(r'[^\\w\\s@.,!?#<>:;]', '', text, flags=re.UNICODE)\n",
        "    \n",
        "    # Step 1d: Remove leading punctuation (like . , ! ?) at start of text\n",
        "    text = re.sub(r'^[.,!?_\\s]+', '', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "# Step 2: Define helper function for cleaning datasets\n",
        "def clean_dataset(input_path, output_path, text_column='tweet'):\n",
        "    \"\"\"\n",
        "    Reads a dataset, cleans the specified text column, and saves cleaned dataset.\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(input_path)\n",
        "\n",
        "    if text_column not in df.columns:\n",
        "        raise ValueError(f\"Column '{text_column}' not found in dataset.\")\n",
        "\n",
        "    df['clean_text'] = df[text_column].apply(clean_text)\n",
        "\n",
        "    df.to_excel(output_path, index=False)\n",
        "    print(f\"Cleaned dataset saved to {output_path}\")\n",
        "\n",
        "\n",
        "# Step 3: Clean train, validation, and test datasets\n",
        "datasets = [\n",
        "    (\"Constraint_English_Train.xlsx\", \"Constraint_English_Train_Cleaned.xlsx\"),\n",
        "    (\"Constraint_English_Val_GR.xlsx\", \"Constraint_English_Val_Cleaned.xlsx\"),\n",
        "    (\"Constraint_English_Test_GR.xlsx\", \"Constraint_English_Test_Cleaned.xlsx\")\n",
        "]\n",
        "\n",
        "for input_path, output_path in datasets:\n",
        "    clean_dataset(input_path, output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Remove Stopwords\n",
        "- use nltk library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/zara/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/zara/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned dataset saved to Constraint_English_Train_Cleaned.xlsx\n",
            "Cleaned dataset saved to Constraint_English_Val_Cleaned.xlsx\n",
            "Cleaned dataset saved to Constraint_English_Test_Cleaned.xlsx\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/ctip/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/opt/anaconda3/envs/ctip/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.6 Lemmatization\n",
        "- Lemmatization cleans word forms that has \"-ing\" so that words are in keyword forms.\n",
        "- uses Spacy or NLTK\n",
        "- choice: Spacy (more accurate and faster, but larger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc if token.lemma_ != \"-PRON-\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Spelling Correction\n",
        "- Tools available: TextBlob, SymSpell, Hunspell or pyspellchecker\n",
        "- Choice: TextBlob is slow for large datasets, but since our datasets are small it is suitable for the ML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def correct_spelling(text):\n",
        "    return str(TextBlob(text).correct())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/zara/miniconda3/envs/ctip/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt', download_dir=\"/Users/zara/nltk_data\")\n",
        "nltk.data.path.append(\"/Users/zara/miniconda3/envs/ctip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- conda info --envs\n",
        "- replace /Users..... to your own path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Improved final code\n",
        "- added hashtag, link and user mention normalization, word lemetization (+ removed spaces due to lemetization), and spell checking.\n",
        "\n",
        "Tools:\n",
        "1. pandas - reading and writing excel datasets (pd.red_excel, df.to_excel)\n",
        "2. re (regex) - Text normalization (usernames, hashtags, links) and symbol removal.\n",
        "3. nltk - stopword removal \n",
        "4. spacy - lemmatization \n",
        "5. textblob - spelling correction\n",
        "6. lambda + regex - placeholder integrity\n",
        "\n",
        "\n",
        "Preprocessing steps taken:\n",
        "- username to <USER>\n",
        "- hashatags to <HASHTAG:topic>\n",
        "- links to <LINK>\n",
        "- removed underscores\n",
        "- removed emojis/weird symbols \n",
        "- removed leading punctuations\n",
        "- removed stopwords - split text intp tokens, filter stopwords and rejoin\n",
        "- lemmatization - convert words to their root form (avoid pronouns)\n",
        "- remove spaces in placeholders after lemmetization\n",
        "- spelling correction \n",
        "\n",
        "Dataset Processing\n",
        "- Reads each dataset (pd.read_excel)\n",
        "- Applies cleaning to a specified column (tweet)\n",
        "- Writes cleaned data to a new Excel file.\n",
        "- Processes multiple datasets in a loop.\n",
        "\n",
        "\n",
        "Observations: there are still some spaces in placeholders <> after lemmetization even when spaces are removed with regex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/zara/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/opt/anaconda3/envs/ctip/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned dataset saved to Constraint_English_Train_Cleaned.xlsx\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/ctip/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned dataset saved to Constraint_English_Val_Cleaned.xlsx\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/ctip/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned dataset saved to Constraint_English_Test_Cleaned.xlsx\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Lemmatize\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc if token.lemma_ != \"-PRON-\"])\n",
        "\n",
        "# Remove spaces from lemmaitization\n",
        "def remove_spaces_in_placeholders(text):\n",
        "    return re.sub(r'<\\s*(.*?)\\s*>', lambda m: f\"<{m.group(1).replace(' ', '')}>\", text)\n",
        "\n",
        "# Spelling correction\n",
        "def correct_spelling(text):\n",
        "    return str(TextBlob(text).correct())\n",
        "\n",
        "# Hashtag normalization function\n",
        "def replace_hashtag(match):\n",
        "    hashtag = match.group()[1:].lower()\n",
        "    return f\"<HASHTAG:{hashtag}>\"\n",
        "\n",
        "# Download stopwords once\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "negations = {\"no\", \"not\", \"nor\", \"never\"}\n",
        "stop_words -= negations\n",
        "\n",
        "# Cleaning function\n",
        "def clean_text(text, do_lemmatize=True, do_spellcheck=False):\n",
        "    text = str(text)\n",
        "\n",
        "    # Normalize usernames, hashtags, and links\n",
        "    text = re.sub(r'@\\w+', '<USER>', text)\n",
        "    text = re.sub(r'#\\w+', replace_hashtag, text)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '<LINK>', text)\n",
        "\n",
        "    # Remove underscores\n",
        "    text = text.replace('_', '')\n",
        "\n",
        "    # Remove emojis/unusual symbols but keep punctuation\n",
        "    text = re.sub(r'[^\\w\\s@.,!?#<>:;]', '', text, flags=re.UNICODE)\n",
        "\n",
        "    # Remove leading punctuation\n",
        "    text = re.sub(r'^[.,!?_\\s]+', '', text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    text = \" \".join(tokens)\n",
        "\n",
        "    # Lemmatize and remove spaces \n",
        "    if do_lemmatize:\n",
        "        text = lemmatize_text(text)\n",
        "    text = remove_spaces_in_placeholders(text)\n",
        "\n",
        "    # Spellcheck\n",
        "    if do_spellcheck:\n",
        "        text = correct_spelling(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Dataset cleaning helper\n",
        "def clean_dataset(input_path, output_path, text_column='tweet', do_lemmatize=True, do_spellcheck=False):\n",
        "    df = pd.read_excel(input_path)\n",
        "\n",
        "    if text_column not in df.columns:\n",
        "        raise ValueError(f\"Column '{text_column}' not found in dataset.\")\n",
        "\n",
        "    df['clean_text'] = df[text_column].apply(lambda x: clean_text(x, do_lemmatize, do_spellcheck))\n",
        "\n",
        "    df.to_excel(output_path, index=False)\n",
        "    print(f\"Cleaned dataset saved to {output_path}\")\n",
        "\n",
        "# Run cleaning for datasets\n",
        "datasets = [\n",
        "    (\"Constraint_English_Train_GR.xlsx\", \"Constraint_English_Train_Cleaned.xlsx\"),\n",
        "    (\"Constraint_English_Val_GR.xlsx\", \"Constraint_English_Val_Cleaned.xlsx\"),\n",
        "    (\"Constraint_English_Test_GR.xlsx\", \"Constraint_English_Test_Cleaned.xlsx\")\n",
        "]\n",
        "\n",
        "for input_path, output_path in datasets:\n",
        "    clean_dataset(input_path, output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ctip",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
